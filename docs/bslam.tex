\documentclass{article}
\iffalse
\usepackage{authblk}
\fi


\title { A Dense Optical Flow based Approach for Real Time Mapping and Localization }
\author{   }
\iffalse
\author{Gao Qian\thanks{gaoqian@buaa.edu.cn}}
\affil{Department of Computer Science, BUAA University}
\fi

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}
A dense optical flow method for real time mapping and localization is produced in this article.

\section{Related Works}
\subsection{Dense Optical Flow}
Dense optical flow research started more than 30 years ago with the work of Horn and Schunck \cite{Horn1981Determining}. We refer to publications like \cite{Baker2007A} \cite{Sun2010Secrets} \cite{Vogel2013An} for detail overview of optical flow methods and the general principles behind it.\par

Some works that integrated sparse descriptor matching for improved large displacement performance \cite{Brox2011Large} \cite{Xu2012Motion} \cite{Weinzaepfel2014DeepFlow} \cite{Kennedy2015Optical} \cite{Timofte2015Sparse}.\par

End-to-end optical flow estimation with convolutional networks was proposed by Dosovitskiy et al. in \cite{Dosovitskiy2015FlowNet}. Their model, dubbed FlowNet, takes a pair of images as input and outputs the flow field. Following FlowNet, several papers have studied optical flow estimation with CNNs: featuring a 3D convolutional network \cite{Du2015Deep}, an unsupervised learning objective \cite{Ahmadi2016Unsupervised} \cite{Yu2016Back}, carefully designed rotationally invariant architectures \cite{Teney2016Learning}, or a pyramidal approach based on the coarse-to-fine idea of variational methods \cite{Ranjan2017Optical}.\par

An alternative approach to learning-based optical flow estimation is to use CNNs for matching image patches. Thewlis et al. [30] formulate Deep Matching [32] as a convolutional network and optimize it end-to-end. Gadot and Wolf [13] and Bailer et al. [3] learn image patch descriptors using Siamese network architectures. These methods can reach good accuracy, but require exhaustive matching of patches. Thus, they are restrictively slow for most practical applications. Moreover, patch based approaches lack the possibility to use the larger context of the whole image because they operate on small image patches.


\subsection{SfM from Optical Flow}

SfM and optical flow have both made significant, but mostly independent, progress. Roughly speaking, SfM methods require purely rigid scenes and use sparse point matches, wide baselines between frames, solve for accurate camera intrinsics and extrinsics, and exploit bundle adjustment to optimize over many views at once. In contrast, optical flow is applied to scenes containing generic motion, exploits continuous optimization, makes weak assumptions about the scene (e.g. that it is piecewise smooth), and typically processes only pairs of video frames at a time.\par

There have been many attempts to combine SfM and flow methods, dating to the 80’s\cite{Heeger1992Subspace}. For video sequences from narrow-focallength lenses, the estimation of the camera motion is challenging, as it is easy to confuse translation with rotation and difficult to estimate the camera intrinsics\cite{Horn1988Direct}.\par

More recently there have been attempts to combine SfM and optical flow \cite{Bai2016Exploiting} \cite{Oisel2001Epipolar} \cite{Valgaerts2008A} \cite{Wedel2009Structure} \cite{Yuan2007Detecting}. The top monocular optical flow method on the KITTI-2012 benchmark estimates the fundamental matrix and computes flow along the epipolar lines [40]. This approach is limited to fully rigid scenes. Wedel et al. [39] compute the fundamental matrix and regularize optical flow to lie along the epipolar lines. If they detect independent motion, they revert to standard optical flow for the entire frame. 


Agrawal et al. \cite{Agrawal2015Learning} and Jayaraman \cite{Jayaraman2015Learning} applied ConvNets to estimating camera motion. The main focus of these works is not on the camera motion itself, but on learning a feature representation useful for recognition. The accuracy of the estimated camera motion is not competitive with classic methods. Kendall et al. \cite{Kendall2015Modelling} trained a ConvNet for camera relocalization — predicting the location of the camera within a known scene from a single image. This is mainly an instance recognition task and requires retraining for each new scene. All these works do not provide depth estimates.

\subsection{Calibration and Dense Geometry Estimation}

It is well established that 3D reconstruction and camera estimation are tightly linked together, bundle adjustment problems being a good example of how calibration can be improved by jointly estimating the 3D structure and the camera parameters. Surprisingly, until recently, dense surface reconstruction was only considered as a next and/or independent step from the calibration problem. It would be more elegant if one could directly minimize the photometric reprojection
error to estimate both shape and camera parameters (and eventually the scene radiance) at the same time.\par

Georgel et al. \cite{Georgel2008A} propose a unified framework to combine both the geometric and photometric information. As both terms are not homogeneous, it is not clear how to combine and weight them efficiently. In this work, we propose to use the photometric information only, assuming an initial calibration is already provided. It is also worth mentioning the work of [5], which estimates 3D oriented patches, and then minimizes the reprojection error to refine both patches and camera parameters. They show substantial improvements
in accuracy for 3D reconstruction, hence showing a photometric-based refinement of the calibration is necessary for high quality multi-view stereo. Both \cite{Georgel2008A} and \cite{Furukawa2008Accurate} assume the surface can be represented by planar local patches. Here, we represent the surface as an arbitrary triangular mesh. Real-time structure-from-motion is also possible by using dense tracking and mapping \cite{Newcombe2011DTAM}.


\subsection{Dense Pipeline for 3D Reconstruction from Image Sequences}

Monocular tracking and range image integration are most closely related to our work. A prominent tracking approach is the PTAM algorithm by Klein and Murray \cite{Klein2007Parallel}, which tracks sparse features of the scene and creates a map of the environment. With this map the new camera pose is estimated depending on the matched features. Newcombe et al. \cite{Newcombe2011DTAM} follow a similar tracking approach, but use a dense scene model instead of a sparse feature map. This way, they achieve a higher robustness to rapid motion. However, they still need standard point features to initialize their tracking algorithm. In order to describe such point features one can employ the wellknown SIFT descriptor by Lowe \cite{Lowe2004Distinctive}, or one of the many modifications such as SURF \cite{Bay2006SURF} or GLOH \cite{Mikolajczyk2005A}.\par

Instead of using RGB images for tracking, also range images can be successfully employed for this task. Newcombe et al. \cite{Newcombe2011KinectFusion} present a tracking algorithm based on dense depth frame alignment, and Izadi et al. \cite{Izadi2011KinectFusion} investigate a similar approach that focuses on reconstruction. Zhou et al. use points of interest to reconstruct a dense model from range images \cite{Zhou2013Dense}. They employ a global optimization scheme which protects parts of the scene that have been scanned already. This leads to more consistent and detailed reconstructions. The disadvantage of depth sensors is their typically lower resolution compared to RGB images. Moreover, these algorithms are not applicable in certain situations such as the reconstruction from old image sequences or benchmarks where no depth data is available.\par


\subsection{Bundle Adjustment for Dense Multi-View 3D Modeling}

In recent decades, dense geometry recovery has lead to a large number of efficient techniques in order to obtain dense and accurate 3D models, e.g. see \cite{Seitz2006A} \cite{Strecha2008On} for a comparison of recent approaches in the context of multi-view stereo. While some algorithms are based on dense features or patches \cite{Furukawa2008Accurate} others are based on energy minimization techniques. Among those techniques, variational methods have become popular. They differ from the kind of energy they minimize, the way they minimize it or the surface representation they choose. For example \cite{Pons2007Multi} uses the Level Set framework using a global image score, \cite{Kolev2009Continuous} uses a convex formulation minimizing a photometric error defined over a discretized grid. In \cite{Delaunoy2011Gradient} and \cite{Vu2012High}, a mesh refinement technique is proposed, minimizing a photometric cost measure. While all those methods return good results in recovering the 3D shape, only a few of them address the problem of camera calibration from dense data. In the following, we describe related work regarding efforts in joint calibration/geometry estimation focusing on the resolution of reprojection error functionals.



%\begin{enumerate}
%\item
%\end{enumerate}



\section{Method}
\subsection{Dense Optical Flow Calculation}
There are three streams to accomplish this task. First make directional dense optical flow.

\bibliographystyle{unsrt}
\bibliography{docs/bslam}

%\begin{thebibliography}{1}

%\bibitem{power}
%aaaaabbbb

%\end{thebibliography}



\end{document}

%\cite{einstein} 
%\bibliography{docs/bslam}

